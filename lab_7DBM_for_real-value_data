import torch
from torchvision import datasets, transforms


transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))  # Standardize
])

train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST('./data', train=False, transform=transform)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)
class GaussianBernoulliRBM(torch.nn.Module):
    def __init__(self, visible_dim, hidden_dim, lr=0.01, k=1):
        super().__init__()
        self.W = torch.nn.Parameter(torch.randn(hidden_dim, visible_dim) * 0.01)
        self.v_bias = torch.nn.Parameter(torch.zeros(visible_dim))
        self.h_bias = torch.nn.Parameter(torch.zeros(hidden_dim))
        self.lr = lr
        self.k = k

    def sample_h(self, v):
        p_h = torch.sigmoid(torch.matmul(v, self.W.t()) + self.h_bias)
        return p_h, torch.bernoulli(p_h)

    def sample_v(self, h):
    
        mean_v = torch.matmul(h, self.W) + self.v_bias
        return mean_v, torch.normal(mean_v, 1.0)

    def contrastive_divergence(self, v0):
        vk = v0
        for _ in range(self.k):
            p_hk, hk = self.sample_h(vk)
            p_vk, vk = self.sample_v(hk)
                positive = torch.matmul(v0.t(), p_hk)
        negative = torch.matmul(vk.t(), p_hk)
        grad_W = (positive - negative).t() / v0.size(0)
        grad_v = (v0 - vk).mean(0)
        grad_h = (p_hk - self.sample_h(vk)[0]).mean(0)

        self.W.data += self.lr * grad_W
        self.v_bias.data += self.lr * grad_v
        self.h_bias.data += self.lr * grad_h
        return torch.mean((v0 - vk)**2)
visible_dim = 784
hidden1_dim = 500
rbm1 = GaussianBernoulliRBM(visible_dim, hidden1_dim, lr=0.01, k=1)


for epoch in range(10):
    epoch_error = 0.0
    for data, _ in train_loader:
        data = data.view(-1, 784)
        error = rbm1.contrastive_divergence(data)
        epoch_error += error.item()
    print(f"Epoch {epoch+1}, Error: {epoch_error / len(train_loader):.4f}")
def extract_features(rbm, dataloader):
    features = []
    labels = []
    with torch.no_grad():
        for data, label in dataloader:
            data = data.view(-1, 784)
            p_h, _ = rbm.sample_h(data)
            features.append(p_h.numpy())
            labels.append(label.numpy())
    return np.concatenate(features), np.concatenate(labels)

train_features, train_labels = extract_features(rbm1, train_loader)
test_features, test_labels = extract_features(rbm1, test_loader)
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

clf = LogisticRegression(max_iter=1000)
clf.fit(train_features, train_labels)
pred_labels = clf.predict(test_features)
accuracy = accuracy_score(test_labels, pred_labels)
print(f"Test Accuracy: {accuracy:.4f}")
def compute_reconstruction_error(rbm, dataloader):
    total_error = 0.0
    with torch.no_grad():
        for data, _ in dataloader:
            data = data.view(-1, 784)
            p_h, _ = rbm.sample_h(data)
            p_v, _ = rbm.sample_v(p_h)
            error = torch.mean((data - p_v)**2)
            total_error += error.item()
    return total_error / len(dataloader)

recon_error = compute_reconstruction_error(rbm1, test_loader)
print(f"Reconstruction Error: {recon_error:.4f}")
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

tsne = TSNE(n_components=2)
train_tsne = tsne.fit_transform(train_features[:1000])

plt.scatter(train_tsne[:,0], train_tsne[:,1], c=train_labels[:1000], cmap='tab10')
plt.colorbar()
plt.show()
